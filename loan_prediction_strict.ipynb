{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72484eb2",
   "metadata": {},
   "source": [
    "# Loan Prediction\n",
    "\n",
    "### Developed by:\n",
    "\n",
    "1. Tiago Pinheiro - 202205295\n",
    "2. Tiago Rocha    - 202005428\n",
    "3. Vasco Melo     - 202207564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('data/credit_risk_dataset.csv')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_age_entries = dataset[dataset['person_age'] > 120]\n",
    "print(\"Entries with person_age > 120:\")\n",
    "print(removed_age_entries)\n",
    "\n",
    "# Find entries where person_emp_length > person_age\n",
    "removed_emp_length_entries = dataset[dataset['person_emp_length'] > dataset['person_age']]\n",
    "print(\"\\nEntries with person_emp_length > person_age:\")\n",
    "print(removed_emp_length_entries)\n",
    "\n",
    "# Combine all removed entries for reference\n",
    "all_removed_entries = pd.concat([removed_age_entries, removed_emp_length_entries]).drop_duplicates()\n",
    "print(\"\\nAll entries to be removed:\")\n",
    "print(all_removed_entries)\n",
    "\n",
    "# Remove invalid entries from the dataset\n",
    "dataset = dataset[dataset['person_age'] <= 120]\n",
    "dataset = dataset[dataset['person_emp_length'] <= dataset['person_age']]\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"\\nDataset after removing invalid entries:\")\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43939e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid entries from the dataset\n",
    "dataset = dataset[dataset['person_age'] <= 120]\n",
    "dataset = dataset[dataset['person_emp_length'] <= dataset['person_age']]\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"\\nDataset after removing invalid entries:\")\n",
    "display(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incomplete data (missing values)\n",
    "print(\"Incomplete data (missing values) in the dataset:\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_data = dataset.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "if not missing_data.empty:\n",
    "    print(missing_data)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in the dataset\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(\"Dataset after removing rows with missing values:\")\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to convert 'person_home_ownership' to numeric values\n",
    "home_ownership_map = {\n",
    "    'MORTGAGE': 0,\n",
    "    'RENT': 1,\n",
    "    'OWN': 2,\n",
    "    'OTHER': 3\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'person_home_ownership' column\n",
    "dataset['person_home_ownership'] = dataset['person_home_ownership'].map(home_ownership_map)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Transformed 'person_home_ownership' column:\")\n",
    "print(dataset['person_home_ownership'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653505b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to convert 'loan_intent' to numeric values\n",
    "loan_intent_map = {\n",
    "    'VENTURE': 0,\n",
    "    'EDUCATION': 1,\n",
    "    'DEBTCONSOLIDATION': 2,\n",
    "    'HOMEIMPROVEMENT': 3,\n",
    "    'MEDICAL': 4,\n",
    "    'PERSONAL': 5\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'loan_intent' column\n",
    "dataset['loan_intent'] = dataset['loan_intent'].map(loan_intent_map)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Transformed 'loan_intent' column:\")\n",
    "print(dataset['loan_intent'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to convert 'loan_grade' to numeric values\n",
    "loan_grade_map = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'loan_grade' column\n",
    "dataset['loan_grade'] = dataset['loan_grade'].map(loan_grade_map)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Transformed 'loan_grade' column:\")\n",
    "print(dataset['loan_grade'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13802738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to convert 'cb_person_default_on_file' to numeric values\n",
    "cb_person_default_map = {\n",
    "    'Y': 1,\n",
    "    'N': 0\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'cb_person_default_on_file' column\n",
    "dataset['cb_person_default_on_file'] = dataset['cb_person_default_on_file'].map(cb_person_default_map)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Transformed 'cb_person_default_on_file' column:\")\n",
    "print(dataset['cb_person_default_on_file'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = [col for col in dataset.columns if col != 'id']\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sb.pairplot(dataset[columns_to_plot].dropna(), hue='loan_status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "columns_to_plot = [col for col in dataset.columns if col != 'loan_status']\n",
    "\n",
    "num_columns = len(columns_to_plot)\n",
    "rows = (num_columns + 1) // 2  \n",
    "\n",
    "for column_index, column in enumerate(columns_to_plot):\n",
    "    plt.subplot(rows, 2, column_index + 1) \n",
    "    sb.violinplot(x='loan_status', y=column, data=dataset)\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets with the same distribution of loan_status\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform stratified sampling based on 'loan_status'\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, \n",
    "    test_size=0.25,  # 25% for testing\n",
    "    random_state=1,  # For reproducibility\n",
    "    stratify=dataset['loan_status']  # Maintain the same distribution of 'loan_status'\n",
    ")\n",
    "\n",
    "original_percentage = (dataset['loan_status'].value_counts(normalize=True) * 100).loc[1]\n",
    "print(\"Original dataset distribution:\")\n",
    "print(f\"Percentage of 1 in original dataset: {original_percentage:.2f}%\")\n",
    "\n",
    "# Print the percentage of 1 in the 'loan_status' column for the training dataset\n",
    "train_percentage = (train_dataset['loan_status'].value_counts(normalize=True) * 100).loc[1]\n",
    "print(f\"Percentage of 1 in training dataset: {train_percentage:.2f}%\")\n",
    "\n",
    "# Print the percentage of 1 in the 'loan_status' column for the testing dataset\n",
    "test_percentage = (test_dataset['loan_status'].value_counts(normalize=True) * 100).loc[1]\n",
    "print(f\"Percentage of 1 in testing dataset: {test_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and testing datasets to CSV files\n",
    "train_dataset.to_csv('data/train.csv', index=False)\n",
    "test_dataset.to_csv('data/test.csv', index=False)\n",
    "\n",
    "print(\"Training dataset saved as 'train.csv'.\")\n",
    "print(\"Testing dataset saved as 'test.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separate features (X) and target (y) for training and testing datasets\n",
    "X_train = train_dataset.drop(columns=['loan_status'])\n",
    "y_train = train_dataset['loan_status']\n",
    "X_test = test_dataset.drop(columns=['loan_status'])\n",
    "y_test = test_dataset['loan_status']\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Decision Tree model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Decision Tree model 1000 times with different splits and display a histogram of accuracies\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Store accuracies for each run\n",
    "accuracies = []\n",
    "\n",
    "# Run the model 1000 times\n",
    "for i in range(1000):\n",
    "    # Split the dataset into training and testing sets with stratified sampling\n",
    "    train_dataset, test_dataset = train_test_split(\n",
    "        dataset,\n",
    "        test_size=0.25,  # 25% for testing\n",
    "        random_state=i,  # Change random state for each iteration\n",
    "        stratify=dataset['loan_status']  # Maintain the same distribution of 'loan_status'\n",
    "    )\n",
    "    \n",
    "    # Separate features (X) and target (y) for training and testing datasets\n",
    "    X_train = train_dataset.drop(columns=['loan_status'])\n",
    "    y_train = train_dataset['loan_status']\n",
    "    X_test = test_dataset.drop(columns=['loan_status'])\n",
    "    y_test = test_dataset['loan_status']\n",
    "    \n",
    "    # Initialize the Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=i)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and display the average accuracy over 1000 runs\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average accuracy over 1000 runs: {average_accuracy:.2f}\")\n",
    "\n",
    "# Plot a histogram of the accuracies\n",
    "plt.hist(accuracies, bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Model Accuracies')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparação dos dados para análise\n",
    "credit_risk_dataset = pd.read_csv('data/credit_risk_dataset.csv')\n",
    "all_inputs = credit_risk_dataset[\n",
    "    [\n",
    "        'person_age',\n",
    "        'person_income',\n",
    "        'person_home_ownership',\n",
    "        'person_emp_length',\n",
    "        'loan_intent',\n",
    "        'loan_grade',\n",
    "        'loan_amnt',\n",
    "        'loan_int_rate',\n",
    "        'loan_percent_income',\n",
    "        'cb_person_cred_hist_length'\n",
    "    ]\n",
    "].values\n",
    "\n",
    "loan_status = credit_risk_dataset['loan_status'].values\n",
    "all_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96242ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divisão dos dados\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, loan_status, test_size=0.25, random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_project_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
